<?xml version="1.0" encoding="UTF-8"?>
<!-- 
   Copyright 2014 Norconex Inc.

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

<!-- This self-documented configuration file is meant to be used as a reference
     or starting point for a new configuration. 
     It contains all core features offered in this release.  Sometimes
     multiple implementations are available for a given feature. Refer 
     to site documentation for more options and complete description of 
     each features.
     -->
<fscollector id="filesystemcollector-template">

  <!-- Variables: Optionally define variables in this configuration file
       using the "set" directive, or by using a file of the same name
       but with the extension ".variables" or ".properties".  Refer 
       to site documentation to find out what each extension does.
       Finally, one can pass an optional variable file when starting the
       crawler.  The following is good practice to reference frequently 
       used classes in a shorter way.
       -->
  #set($workdir = "c:\path\to\your\workdir")

  #set($core      = "com.norconex.collector.core")
  #set($file      = "com.norconex.collector.fs")
  #set($committer = "com.norconex.committer")

  #set($filterExtension   = "${core}.filter.impl.ExtensionReferenceFilter")
  #set($filterRegexRef    = "${core}.filter.impl.RegexReferenceFilter")
  #set($filterRegexMeta   = "${core}.filter.impl.RegexMetadataFilter")
  #set($metaChecksummer   = "${file}.checksum.impl.FileMetadataChecksummer")
  #set($docChecksummer    = "${core}.checksum.impl.MD5DocumentChecksummer")
  #set($dataStoreFactory  = "${core}.data.store.impl.mapdb.MapDBCrawlDataStoreFactory")

  <!-- Location where internal progress files are stored. -->
  <progressDir>$workdir\progress</progressDir>

  <!-- Location where logs are stored. -->
  <logsDir>$workdir\logs</logsDir>

  <!-- All crawler configuration options can be specified as default 
       (including start paths).  Settings defined here will be inherited by 
       all individual crawlers defined further down, unless overwritten.
       -->
  <crawlerDefaults>

    <!-- Crawler "work" directory.  This is where files dowloaded or created as
         part of crawling activities (besides logs and progress) get stored.
         It should be unique to each crawlers.
         -->
    <workDir>$workdir</workDir>

    
    <!-- Mandatory starting path(s) where crawling begins.  If you put more 
         than one path, they will be processed together.  -->    
    <startPaths>
      <path>c:\path\to\files\to\crawl</path>
      <pathsFile>c:\path\to\a\file\full\of\start\paths.xml</pathsFile>
    </startPaths>

    <!-- How many threads you want a crawler to use. Default is 2 threads.
      -->
    <numThreads>2</numThreads>

    <!-- Stop crawling after how many successfully processed files.  
         A successful file is one that is either new or modified, that was 
         not rejected, not deleted, or did not generate any error.  As an
         example, this is a file that will end up in your target data 
         repository (e.g,. search engine). 
         Default is -1 (unlimited)
          -->
    <maxDocuments>-1</maxDocuments>

    <!-- Keep downloaded files. Default is false (deletes them after they have
         been processed).
         -->
    <keepDownloads>false</keepDownloads>

    <!-- What to do with orphan documents.  Orphans are valid 
         documents, which on subsequent crawls can no longer be reached when 
         running the crawler (e.g. there are no links pointing to that page 
         anymore).  Available options are: 
         IGNORE (default), DELETE, and PROCESS.
         -->
    <orphansStrategy>IGNORE</orphansStrategy>

    <!-- One or more optional listeners to be notified on various crawling
         events (e.g. document rejected, document imported, etc). 
         Class must implement 
         com.norconex.collector.core.event.ICrawlerEventListener
         -->
    <crawlerListeners>
      <listener class="YourClass"/>
    </crawlerListeners>

    <!-- Factory class creating a database for storing crawl status and
         other information.  Classes must implement 
         com.norconex.collector.core.data.store.ICrawlURLDatabaseFactory.  
         Default implementation is the following.
         -->
    <crawlDataStoreFactory class="$dataStoreFactory" />

    <!-- Optionally filter a path BEFORE any download. Classes must implement 
         com.norconex.collector.core.filter.IReferenceFilter, 
         like the following examples.
         -->
    <referenceFilters>
      <filter class="$filterExtension" onMatch="exclude" >
        jpg,gif,png,ico,css,js</filter>
      <filter class="$filterRegexRef">/path/to/include/.*</filter>
    </referenceFilters>

    <!-- Optionally filter AFTER obtaining the file properties.  Classes must 
         implement com.norconex.collector.core.filter.IMetadataFilter.  
         -->
    <metadataFilters>
      <filter class="$filterRegexMeta" 
              onMatch="exclude"
              caseSensitive="false"
              field="collector.content-type">.*css.*</filter>
    </metadataFilters>        

    <!-- Generates a checksum value from a file properties to find out if 
         a document has changed. Class must implement
         com.norconex.collector.core.checksum.IMetadataChecksummer.  
         Default implementation is the following. 
         -->
    <metadataChecksummer class="$metaChecksummer" />

    <!--  Optionally filters a document. Classes must implement 
          com.norconex.collector.core.filter.IDocumentFilter-->
    <documentFilters>
        <filter class="YourClass" />
    </documentFilters>

    <!-- Optionally process a document BEFORE importing it. Classes must
         implement com.norconex.collector.http.doc.IHttpDocumentProcessor.
         -->
    <preImportProcessors>
       <processor class="YourClass"></processor>
    </preImportProcessors>

    <!-- Import a document.  This step calls the Importer module.  The
         importer is a different module with its own set of XML configuration
         options.  Please refer to importer for complete documentation.
         Below gives you an overview of the main importer tags.
         -->
    <importer>
        <preParseHandlers>
            <tagger class="..."/>
            <transformer class="..." />
            <filter class="..." />
        </preParseHandlers>
        <documentParserFactory class="..." />
        <postParseHandlers>
            <tagger class="..."/>
            <transformer class="..." />
            <filter class="..." />
        </postParseHandlers>
    </importer>           


    <!-- Create a checksum out of a document to figure out if a document
         has changed, AFTER it has been imported. Class must implement 
         com.norconex.collector.core.checksum.IDocumentChecksummer.
         Default implementation is the following.
         -->
    <documentChecksummer class="$docChecksummer" />

    <!-- Optionally process a document AFTER importing it. Classes must
         implement com.norconex.collector.http.doc.IHttpDocumentProcessor.
         -->
    <postImportProcessors>
       <processor class="YourClass"></processor>
    </postImportProcessors>

    <!-- Commits a document to a data source of your choice.
         This step calls the Committer module.  The
         committer is a different module with its own set of XML configuration
         options.  Please refer to committer for complete documentation.
         Below is an example using the FileSystemCommitter.
         -->
    <committer class="$committer.impl.FileSystemCommitter">
      <directory>$workdir\crawledFiles</directory>
    </committer>

  </crawlerDefaults>


  <!-- Individual crawlers can be defined here.  All crawler default
       configuration settings will apply to all crawlers created unless 
       explicitly overwritten in crawler configuration.
       For configuration options where multiple items can be present 
       (e.g. filters), the whole list will in crawler defaults would be
       overwritten.
       Since the options are the same as the defaults above, the documentation 
       is not repeated here.
       The only difference from "crawlerDefaults" is the addition of the "id"
       attribute on the crawler tag.  The "id" attribute uniquelly identifies
       each of your crawlers.  
       -->
  <crawlers>
    <crawler id="Test Filesystem">
       <!-- Overwrite any defaults here. -->
    </crawler>
  </crawlers>

</fscollector>